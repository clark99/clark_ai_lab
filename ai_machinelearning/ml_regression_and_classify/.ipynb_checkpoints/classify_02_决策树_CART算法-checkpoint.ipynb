{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 决策树算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 决策树简介\n",
    "### 1.1.1 什么是决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 决策树主要有二元分支和多元分支.\n",
    "- 决策树是**判定树**\n",
    "    * 内部结点是决策节点: 对某个属性的一次测试\n",
    "    * 分支: 每条边代表一个测试结果.\n",
    "    * 叶子: 代表某个类或者类的分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 使用决策树进行判别:\n",
    "    * 决策条件-决策路径-叶子(结果)代表分类\n",
    "- 决策树的数学模式**解题思路**:\n",
    "    * 贪心的算法 greedy solution\n",
    "    * 不是最好的树,全局最优解\n",
    "    * 当前的树里面找最好的树,局部最优解."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 决策树的决策依据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 决策树的目标:\n",
    "    * **最快速完成类别的判定**\n",
    "- 直观思路\n",
    "    * 应该凸显这种路径: 最有利做出判别\n",
    "    * **最大减少在类别判定上的不确定性**\n",
    "    * **纯度上升的更快,更快速到达纯度更高的集合**\n",
    "- 怎么选择优先进行决策的判定属性\n",
    "    * 好的特征是什么原理?\n",
    "    * **获得更多信息来减少不确定性**\n",
    "    * **知道的信息越多,信息的不确定性越小**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 信息熵和条件熵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 信息熵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1.1 不确定性\n",
    "- **信息量的度量就等于不确定性的多少**\n",
    "    * 信息熵高:我们一无所知的事，就需要了解大量的信息\n",
    "    * 信息熵低:我们对某件事已经有了较多的了解，我们就不需要太多的信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1.2 信息熵的公式\n",
    "- 对数的运算法则\n",
    "$$\n",
    "log_a(mn)=log_am+log_an\n",
    "$$\n",
    "- 概率的公式\n",
    "$$\n",
    "p(x,y)=p(x)p(y)\n",
    "$$\n",
    "- 两个事件同时发生的信息等于各自信息的和\n",
    "$$\n",
    "I(x,y)=I(x)+I(y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**随机变量 x 的自信息**\n",
    "$$\n",
    "I(x)=-logp(x)\n",
    "$$\n",
    "- 负号是用来保证信息量是正数或者零\n",
    "- 描述的是随机变量的某个事件发生所带来的信息量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**信息熵: 传送一个随机变量传输的平均信息量是$I(x)=-logp(x)$的期望**\n",
    "$$\n",
    "H\\left(X\\right)= -\\sum_{i=1}^{n}p\\left(x_{i}\\right)log\\left(p\\left(x_{i}\\right)\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1.3 信息熵的解读\n",
    "- 随机变量 x 的熵,它是表示随机变量不确定的度量，是对所有可能发生的事件产生的信息量的期望\n",
    "- 随机变量的取值个数越多，状态数也就越多，信息熵就越大，混乱程度就越大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 联合熵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "H(X,Y)=-\\displaystyle\\sum_{x,y}p(x,y)logp(x,y)=-\\sum_{i=1}^{n}\\sum_{j=1}^{m}p(x_i,y_i)logp(x_i,y_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 条件熵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 条件熵 H(Y|X) 表示在已知随机变量 X 的条件下, 随机变量 Y 的不确定性\n",
    "- 条件熵 H(Y|X) 定义为 X 给定条件下,  Y 的条件概率分布的熵对 X 的数学期望\n",
    "- 相当在不同X的信息熵,加上X的值的概率的加权"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3.1 条件熵公式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设X有n个取值\n",
    "$$\n",
    "H(Y|X)=\\sum_{i=1}^{n} p(x_i)H(Y|X=x_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "见识Y有m个取值\n",
    "$$\n",
    "H(Y|X=x_i) = - \\sum_{j=1}^{m} p(y_j|X=x_i)\\log p(y_j|X=x_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以\n",
    "$$\n",
    "H(Y|X)=\\sum_{i=1}^{n} p(x_i)H(Y|X=x_i) \\\\\n",
    "=\\sum_{i=1}^{n} p(x_i)\\left(- \\sum_{j=1}^{m} p(y_j|X=x_i) \\log p(y_j|X=x_i)\\right)\\\\\n",
    "=-\\sum_{i=1}^{n}p(x_i) \\sum_{j=1}^{m} p(y_j|x_i) \\log p(y_j|x_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3.2 H(Y|X)条件熵的理解\n",
    "- 在已知一些信息的情况下，因变量 Y 的不纯度，\n",
    "    * 即在X 的划分下，Y 被分割越来越“纯”的程度，\n",
    "    * 即信息的加入可以降低熵\n",
    "- 条件熵表示在已知随机变量 X 的条件下，Y 的条件概率分布的熵对随机变量 X的数学期望"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 联合熵和条件熵的关系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "H\\left( {Y\\left| X \\right.} \\right) = H\\left( {X,Y} \\right) - H\\left( X \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**引用别人的证明公式为**:\n",
    "\\begin{array}{l}\n",
    "H\\left( {Y\\left| X \\right.} \\right) = H\\left( {X,Y} \\right) - H\\left( X \\right)\\\\\n",
    " =  - \\sum\\limits_{x,y} {P\\left( {x,y} \\right)} \\log P\\left( {x,y} \\right) + \\sum\\limits_x {P\\left( x \\right)} \\log P\\left( x \\right)\\\\\n",
    " =  - \\sum\\limits_{x,y} {P\\left( {x,y} \\right)} \\log P\\left( {x,y} \\right) + \\sum\\limits_x {\\left( {\\sum\\limits_y {P\\left( {x,y} \\right)} } \\right)} \\log P\\left( x \\right)\\\\\n",
    " =  - \\sum\\limits_{x,y} {P\\left( {x,y} \\right)} \\log P\\left( {x,y} \\right) + \\sum\\limits_x {\\sum\\limits_y {P\\left( {x,y} \\right)} } \\log P\\left( x \\right)\\\\\n",
    " =  - \\sum\\limits_{x,y} {P\\left( {x,y} \\right)} \\log \\frac{{P\\left( {x,y} \\right)}}{{P\\left( x \\right)}}\\\\\n",
    " =  - \\sum\\limits_{x,y} {P\\left( {x,y} \\right)} \\log P\\left( {y\\left| x \\right.} \\right)\\\\\n",
    " =  - \\sum\\limits_x {\\sum\\limits_y {P\\left( x \\right)} } P\\left( {y\\left| x \\right.} \\right)\\log P\\left( {y\\left| x \\right.} \\right)\\\\\n",
    " =  - \\sum\\limits_x {P\\left( x \\right)\\sum\\limits_y {P\\left( {y\\left| x \\right.} \\right)} } \\log P\\left( {y\\left| x \\right.} \\right)\\\\\n",
    " = \\sum\\limits_x {P\\left( x \\right)\\left( { - \\sum\\limits_y {P\\left( {y\\left| x \\right.} \\right)} \\log P\\left( {y\\left| x \\right.} \\right)} \\right)} \\\\\n",
    " = \\sum\\limits_x {P\\left( x \\right)H\\left( {Y\\left| {X = x} \\right.} \\right)} \n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 ID3算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 信息增益"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**信息增益表示**\n",
    "- 得知特征X的信息, 使得类Y的信息不确定性(信息熵)减少的程度\n",
    "    * 划分前样本集合D的熵是一定的 ，entroy(前)，\n",
    "    * 使用某个特征A划分数据集D，计算划分后的数据子集的熵 entroy(后)\n",
    "    * 信息增益 =  entroy(前) -  entroy(后)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**信息增益的符合表示**\n",
    "- 特征A对训练数据集D的信息增益$g(D,A)$,定义为集合D的经验熵$H(D)$与特征A给定条件下D的经验条件熵$H(D|A)$之差：\n",
    "$$\n",
    "g(D,A)=H(D)−H(D|A)g(D,A)=H(D)−H(D|A)\n",
    "$$\n",
    "- 考虑条件熵和联合熵的关系\n",
    "$$\n",
    "g(D, A) = H(D) - H(D|A) = H(D) - (H(D,A) - H(A)) = H(D) + H(A) - H(D,A)\n",
    "$$\n",
    "- 这个公式让我们想到集合的交集公式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**信息增益的含义**\n",
    "- 最大减少在类别判定上的不确定性,更快的判定类别\n",
    "- 纯度上升的更快,更快速到达纯度更高的集合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 ID3的算法流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- （1）自上而下贪婪搜索\n",
    "- （2）遍历所有的属性，按照信息增益最大的属性进行分裂\n",
    "- （3）根据分裂属性划分样本\n",
    "- （4）重复上述流程，直至满足条件结束"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 ID3算法的缺陷"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**缺陷1**\n",
    "- 缺点：信息增益偏向取值较多的特征\n",
    "- 原因：当特征的取值较多时，根据此特征划分更容易得到纯度更高的子集，因此划分之后的熵更低，由于划分前的熵是一定的，因此信息增益更大，因此信息增益比较 偏向取值较多的特征\n",
    "\n",
    "**极端情况**\n",
    "- 二维表的主键id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**其他缺陷**\n",
    "- 不能处理连续值属性\n",
    "- 不能处理属性值缺失情况\n",
    "- 不能进行剪枝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 C4.5算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1 信息增益率\n",
    "- 可以理解为: 信息增益率 = 分裂信息将信息增益的标准化\n",
    "- 或者理解为: 信息增益率 = 惩罚参数 * 信息增益"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**分裂信息**:\n",
    "- 之前是把集合类别作为随机变量，现在把某个特征作为随机变量，按照此特征的特征取值对集合D进行划分v类，计算熵$H_A(D)$\n",
    "$$\n",
    "SplitH_{A}\\left(D\\right)= -\\sum_{j=1}^{v}\\frac{|D_{j}|}{D}log\\frac{|D_{j}|}{D}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**信息增益率**\n",
    "$$\n",
    "GainRadion\\left(A\\right)= \\frac{g\\left(A,D\\right)}{SplitH_{A}\\left(D\\right)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2 连续值属性和分裂点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**步骤**:\n",
    "- (1)连续值属性从小到大排序,*每对相邻点的中点作为分裂点*\n",
    "- (2)数据集D中有N个不同的连续值属性值, 产生N-1个分裂点\n",
    "- (3)按照每个分裂点,计算每个二分树的信息增益\n",
    "- (4)取得信息增益最大的分裂点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.3 缺失值处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.3.1 学习过程中-缺失值处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**信息增益**\n",
    "\n",
    "- 计算信息熵,忽略缺失值\n",
    "- 计算信息增益, 乘以未缺失实例的比例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**分裂信息熵**\n",
    "- 缺失值当做正常值处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**分裂时候**\n",
    "\n",
    "- 缺失值实例分配给所有判断节点下面的分支上\n",
    "- 但是每个分支的缺失值实例带一个权重: 该分支的概率(频率估算)\n",
    "- 其他正常实例权重为1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**叶节点定义**\n",
    "- (N/E)形式\n",
    "    * N该叶节点的实例数\n",
    "    * E叶节点中属于其他分类的实例数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.3.2 分类过程-缺失值处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 缺失值该属性的遍历所有的分支\n",
    "    * 该属性的所有分支的概率: 分支的叶子节点的N必上所有N的比值\n",
    "- 因为叶节点是NE的形势.\n",
    "    * 正例概率: N/E\n",
    "    * 反例概率: E/N\n",
    "- 根据分支的概率,叶节点的正例概率反例概率的加权和"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.4 剪枝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.4.1 过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**训练样本中的噪声导致过拟合**\n",
    "- 错误的属性值和标签值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**训练样本中缺乏代表性样本所导致的**\n",
    "- 训练样本过少的时候,模型很容易受到过拟合的影响"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.4.2 预剪枝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "限定树的的最大生长高度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.4.3 后剪枝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**后剪枝的目标**\n",
    "在测试集上定义损失函数,通过剪枝使损失函数在测试集上有所降低"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**步骤**\n",
    "- (1)自底向上遍历每一个非叶子节点, 将当前的非叶子节点剪枝(从树中减去,其下所有的叶节点合并一个节点,代替被剪枝的节点)\n",
    "- (2)计算剪枝前后的损失函数\n",
    "- (3)如果损失函数变小, 则剪枝. 否则则还原.\n",
    "- (4)重复上述过程,遍历所有的节点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**子树的损失函数**\n",
    "$$J(\\tau) = E(\\tau) + \\lambda |\\tau|$$\n",
    "- 带惩罚项\n",
    "\n",
    "**后剪枝的损失函数阈值**\n",
    "$$\n",
    "g(c) = \\frac{E(c) - E(\\tau_c)}{|\\tau_c| - 1}\n",
    "$$\n",
    "$$\n",
    "\\lambda_k = \\min(\\lambda, g(c))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意**\n",
    "- 子树的损失函数不做过多介绍, \n",
    "- 感兴趣可以参考博客:**CART-分类和回归树**\n",
    "- https://blog.csdn.net/guoziqing506/article/details/81675022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 CART算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 基尼不纯度gini impurity\n",
    "- **或者称为基尼指数gini index**\n",
    "- **区别于基尼系数gini coefficient, 两者概念不同**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设有K个类，样本点属于第k类的概率为$p_{k}$，则概率分布的基尼指数定义为：\n",
    "$$\n",
    "G(p)=\\sum_{k=1}^{K}p_k(1-p_k)=1-\\sum_{k=1}^Kp_k^2\n",
    "$$\n",
    "满足的条件:\n",
    "$$\n",
    "\\sum_{k=1}^{K}p_k=1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 基尼指数公式的推导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-logp(x)$进行泰勒展开,$p(x)$的高阶趋于0,忽略高阶项.就得到基尼指数(不纯度)的公式\n",
    "- 基尼不纯度的计算可以看出，它的计算更加方便，\n",
    "- 基尼不纯度是熵的一个近似值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 二分类的基尼指数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于二分类问题，如果样本点属于第一类的概率为p,则概率分布的基尼系数为\n",
    "\n",
    "$$\n",
    "Gini(p)=2p(1-p)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设$C_k$为D中属于第k类的样本子集，则基尼指数为\n",
    "$$\n",
    "Gini(D)=1-\\sum_{k=1}^K(\\frac{|C_k|}{|D|})^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设条件A将样本D切分为D1和D2两个数据子集，则在条件A下的样本D的基尼指数为：\n",
    "$$\n",
    "Gini(D,A)=\\frac{|D_1|}{D}Gini(D_1)+\\frac{|D_2|}{D}Gini(D_2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 CART分类树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "条件A, 将样本D, 切分为D1和D2两个数据子集的gini增益为\n",
    "$$\n",
    "\\Delta Gini(A)=Gini(D)-Gini(D,A)=(1-\\sum_{k=1}^K(\\frac{|C_k|}{|D|})^2)-(\\frac{|D_1|}{D}Gini(D_1)+\\frac{|D_2|}{D}Gini(D_2))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 算法实现步骤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1）计算现有样本D的基尼指数，之后利用样本中每一个特征A，及A的每一个可能取值a，根据A>=a与A<a将样本分为两部分，并计算Gini(D,A)值 \n",
    "- 2）找出对应基尼指数最小Gini(D,A)的最优切分特征及取值，并判断是否切分停止条件，否，则输出最优切分点 \n",
    "- 3）递归调用1）2） \n",
    "- 4）生成CART决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 CART回归树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 CART回归树的概念和公式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (1)训练集: $D = \\{(X_1, y_1), (X_2, y_2), \\dots, (X_n, y_n)\\}$, $Y$是连续变量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (2)输入数据空间$X$划分为m个区域: $\\{R_1, R_2, \\dots, R_m\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (3)然后赋给每个输入空间的区域$R_i$有一个固定的代表输出值$C_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (4)回归树的模型公式:\n",
    "$$\n",
    "f(X) = \\sum_{i = 1}^m C_i I(X \\in R_i)\n",
    "$$\n",
    "     * 如果$X \\in R_i$,则$I=1$,否则 $I=0$\n",
    "     * **含义:**先判断X属于哪个区域，然后返回这个区域的代表值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (5)计算损失函数:\n",
    "    * $R_i$这个区域中的元组的y值的均值\n",
    "$$\n",
    "g_i = \\frac{1}{N_i} \\sum_{X_j \\in R_i} y_j\n",
    "$$\n",
    "    * 某个区域$R_i$回归模型的损失函数\n",
    "$$\n",
    "J(C) = \\sum_{X_j \\in R_i} (f(X_j) - g_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 最小二乘回归树生成算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注**: **参考李航的<机器学习>编写, 更详细内容,请自行搜索资料查看**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (1)选择最优切分变量 j 与切分点 s，求解\n",
    "$$\n",
    "\\min_{j,s}\\left [\\min_{c_1}\\sum_{x_i\\in R_1(j,s)}(y_i-c_1)^2+\\min_{c_2}\\sum_{x_i \\in R_2(j,s)} (y_i-c_2)^2 \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (2)用选定的对 (j,s) 划分区域并决定相应的输出值\n",
    "$$\n",
    "R_1(j,s)=\\{x|x^{(j)} \\le s\\},\\quad R_2(j,s)=\\{x|x^{(j)}\\gt s\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (3)继续对两个子区域调用步骤(1),(2)，直至满足停止条件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (4)将输入空间分为 M 个区域$R_1,R_2,\\cdots,R_M$,生成决策树\n",
    "$$\n",
    "f(x)=\\sum_{m=1}^M \\hat c_m I(x\\in R_m)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
