# 1 决策树算法

## 1.1 决策树简介
### 1.1.1 什么是决策树

- 决策树主要有二元分支和多元分支.
- 决策树是**判定树**
    * 内部结点是决策节点: 对某个属性的一次测试
    * 分支: 每条边代表一个测试结果.
    * 叶子: 代表某个类或者类的分布

- 使用决策树进行判别:
    * 决策条件-决策路径-叶子(结果)代表分类
- 决策树的数学模式**解题思路**:
    * 贪心的算法 greedy solution
    * 不是最好的树,全局最优解
    * 当前的树里面找最好的树,局部最优解.

### 1.1.2 决策树的决策依据

- 决策树的目标:
    * **最快速完成类别的判定**
- 直观思路
    * 应该凸显这种路径: 最有利做出判别
    * **最大减少在类别判定上的不确定性**
    * **纯度上升的更快,更快速到达纯度更高的集合**
- 怎么选择优先进行决策的判定属性
    * 好的特征是什么原理?
    * **获得更多信息来减少不确定性**
    * **知道的信息越多,信息的不确定性越小**

## 1.2 信息熵和条件熵

### 1.2.1 信息熵

#### 1.2.1.1 不确定性
- **信息量的度量就等于不确定性的多少**
    * 信息熵高:我们一无所知的事，就需要了解大量的信息
    * 信息熵低:我们对某件事已经有了较多的了解，我们就不需要太多的信息

#### 1.2.1.2 信息熵的公式
- 对数的运算法则
$$
log_a(mn)=log_am+log_an
$$
- 概率的公式
$$
p(x,y)=p(x)p(y)
$$
- 两个事件同时发生的信息等于各自信息的和
$$
I(x,y)=I(x)+I(y)
$$

**随机变量 x 的自信息**
$$
I(x)=-logp(x)
$$
- 负号是用来保证信息量是正数或者零
- 描述的是随机变量的某个事件发生所带来的信息量

**信息熵: 传送一个随机变量传输的平均信息量是$I(x)=-logp(x)$的期望**
$$
H\left(X\right)= -\sum_{i=1}^{n}p\left(x_{i}\right)log\left(p\left(x_{i}\right)\right)
$$

#### 1.2.1.3 信息熵的解读
- 随机变量 x 的熵,它是表示随机变量不确定的度量，是对所有可能发生的事件产生的信息量的期望
- 随机变量的取值个数越多，状态数也就越多，信息熵就越大，混乱程度就越大

### 1.2.2 联合熵

$$
H(X,Y)=-\displaystyle\sum_{x,y}p(x,y)logp(x,y)=-\sum_{i=1}^{n}\sum_{j=1}^{m}p(x_i,y_i)logp(x_i,y_i)
$$

### 1.2.3 条件熵

- 条件熵 H(Y|X) 表示在已知随机变量 X 的条件下, 随机变量 Y 的不确定性
- 条件熵 H(Y|X) 定义为 X 给定条件下,  Y 的条件概率分布的熵对 X 的数学期望
- 相当在不同X的信息熵,加上X的值的概率的加权

#### 1.2.3.1 条件熵公式

假设X有n个取值
$$
H(Y|X)=\sum_{i=1}^{n} p(x_i)H(Y|X=x_i)
$$

见识Y有m个取值
$$
H(Y|X=x_i) = - \sum_{j=1}^{m} p(y_j|X=x_i)\log p(y_j|X=x_i)
$$

所以
$$
H(Y|X)=\sum_{i=1}^{n} p(x_i)H(Y|X=x_i) \\
=\sum_{i=1}^{n} p(x_i)\left(- \sum_{j=1}^{m} p(y_j|X=x_i) \log p(y_j|X=x_i)\right)\\
=-\sum_{i=1}^{n}p(x_i) \sum_{j=1}^{m} p(y_j|x_i) \log p(y_j|x_i)
$$

#### 1.2.3.2 H(Y|X)条件熵的理解
- 在已知一些信息的情况下，因变量 Y 的不纯度，
    * 即在X 的划分下，Y 被分割越来越“纯”的程度，
    * 即信息的加入可以降低熵
- 条件熵表示在已知随机变量 X 的条件下，Y 的条件概率分布的熵对随机变量 X的数学期望

### 1.2.4 联合熵和条件熵的关系

$$
H\left( {Y\left| X \right.} \right) = H\left( {X,Y} \right) - H\left( X \right)
$$

**引用别人的证明公式为**:
\begin{array}{l}
H\left( {Y\left| X \right.} \right) = H\left( {X,Y} \right) - H\left( X \right)\\
 =  - \sum\limits_{x,y} {P\left( {x,y} \right)} \log P\left( {x,y} \right) + \sum\limits_x {P\left( x \right)} \log P\left( x \right)\\
 =  - \sum\limits_{x,y} {P\left( {x,y} \right)} \log P\left( {x,y} \right) + \sum\limits_x {\left( {\sum\limits_y {P\left( {x,y} \right)} } \right)} \log P\left( x \right)\\
 =  - \sum\limits_{x,y} {P\left( {x,y} \right)} \log P\left( {x,y} \right) + \sum\limits_x {\sum\limits_y {P\left( {x,y} \right)} } \log P\left( x \right)\\
 =  - \sum\limits_{x,y} {P\left( {x,y} \right)} \log \frac{{P\left( {x,y} \right)}}{{P\left( x \right)}}\\
 =  - \sum\limits_{x,y} {P\left( {x,y} \right)} \log P\left( {y\left| x \right.} \right)\\
 =  - \sum\limits_x {\sum\limits_y {P\left( x \right)} } P\left( {y\left| x \right.} \right)\log P\left( {y\left| x \right.} \right)\\
 =  - \sum\limits_x {P\left( x \right)\sum\limits_y {P\left( {y\left| x \right.} \right)} } \log P\left( {y\left| x \right.} \right)\\
 = \sum\limits_x {P\left( x \right)\left( { - \sum\limits_y {P\left( {y\left| x \right.} \right)} \log P\left( {y\left| x \right.} \right)} \right)} \\
 = \sum\limits_x {P\left( x \right)H\left( {Y\left| {X = x} \right.} \right)} 
\end{array}

## 1.3 ID3算法

### 1.3.1 信息增益

**信息增益表示**
- 得知特征X的信息, 使得类Y的信息不确定性(信息熵)减少的程度
    * 划分前样本集合D的熵是一定的 ，entroy(前)，
    * 使用某个特征A划分数据集D，计算划分后的数据子集的熵 entroy(后)
    * 信息增益 =  entroy(前) -  entroy(后)

**信息增益的符合表示**
- 特征A对训练数据集D的信息增益$g(D,A)$,定义为集合D的经验熵$H(D)$与特征A给定条件下D的经验条件熵$H(D|A)$之差：
$$
g(D,A)=H(D)−H(D|A)g(D,A)=H(D)−H(D|A)
$$
- 考虑条件熵和联合熵的关系
$$
g(D, A) = H(D) - H(D|A) = H(D) - (H(D,A) - H(A)) = H(D) + H(A) - H(D,A)
$$
- 这个公式让我们想到集合的交集公式

**信息增益的含义**
- 最大减少在类别判定上的不确定性,更快的判定类别
- 纯度上升的更快,更快速到达纯度更高的集合

### 1.3.2 ID3的算法流程

- （1）自上而下贪婪搜索
- （2）遍历所有的属性，按照信息增益最大的属性进行分裂
- （3）根据分裂属性划分样本
- （4）重复上述流程，直至满足条件结束

### 1.3.3 ID3算法的缺陷

**缺陷1**
- 缺点：信息增益偏向取值较多的特征
- 原因：当特征的取值较多时，根据此特征划分更容易得到纯度更高的子集，因此划分之后的熵更低，由于划分前的熵是一定的，因此信息增益更大，因此信息增益比较 偏向取值较多的特征

**极端情况**
- 二维表的主键id

**其他缺陷**
- 不能处理连续值属性
- 不能处理属性值缺失情况
- 不能进行剪枝

## 1.4 C4.5算法

### 1.4.1 信息增益率
- 可以理解为: 信息增益率 = 分裂信息将信息增益的标准化
- 或者理解为: 信息增益率 = 惩罚参数 * 信息增益

**分裂信息**:
- 之前是把集合类别作为随机变量，现在把某个特征作为随机变量，按照此特征的特征取值对集合D进行划分v类，计算熵$H_A(D)$
$$
SplitH_{A}\left(D\right)= -\sum_{j=1}^{v}\frac{|D_{j}|}{D}log\frac{|D_{j}|}{D}
$$

**信息增益率**
$$
GainRadion\left(A\right)= \frac{g\left(A,D\right)}{SplitH_{A}\left(D\right)}
$$

### 1.4.2 连续值属性和分裂点

**步骤**:
- (1)连续值属性从小到大排序,*每对相邻点的中点作为分裂点*
- (2)数据集D中有N个不同的连续值属性值, 产生N-1个分裂点
- (3)按照每个分裂点,计算每个二分树的信息增益
- (4)取得信息增益最大的分裂点

### 1.4.3 缺失值处理

#### 1.4.3.1 学习过程中-缺失值处理

**信息增益**

- 计算信息熵,忽略缺失值
- 计算信息增益, 乘以未缺失实例的比例

**分裂信息熵**
- 缺失值当做正常值处理

**分裂时候**

- 缺失值实例分配给所有判断节点下面的分支上
- 但是每个分支的缺失值实例带一个权重: 该分支的概率(频率估算)
- 其他正常实例权重为1

**叶节点定义**
- (N/E)形式
    * N该叶节点的实例数
    * E叶节点中属于其他分类的实例数

#### 1.4.3.2 分类过程-缺失值处理

- 缺失值该属性的遍历所有的分支
    * 该属性的所有分支的概率: 分支的叶子节点的N必上所有N的比值
- 因为叶节点是NE的形势.
    * 正例概率: N/E
    * 反例概率: E/N
- 根据分支的概率,叶节点的正例概率反例概率的加权和

### 1.4.4 剪枝

#### 1.4.4.1 过拟合

**训练样本中的噪声导致过拟合**
- 错误的属性值和标签值

**训练样本中缺乏代表性样本所导致的**
- 训练样本过少的时候,模型很容易受到过拟合的影响

#### 1.4.4.2 预剪枝

限定树的的最大生长高度

#### 1.4.4.3 后剪枝

**后剪枝的目标**
在测试集上定义损失函数,通过剪枝使损失函数在测试集上有所降低

**步骤**
- (1)自底向上遍历每一个非叶子节点, 将当前的非叶子节点剪枝(从树中减去,其下所有的叶节点合并一个节点,代替被剪枝的节点)
- (2)计算剪枝前后的损失函数
- (3)如果损失函数变小, 则剪枝. 否则则还原.
- (4)重复上述过程,遍历所有的节点

**子树的损失函数**
$$J(\tau) = E(\tau) + \lambda |\tau|$$
- 带惩罚项

**后剪枝的损失函数阈值**
$$
g(c) = \frac{E(c) - E(\tau_c)}{|\tau_c| - 1}
$$
$$
\lambda_k = \min(\lambda, g(c))
$$

**注意**
- 子树的损失函数不做过多介绍, 
- 感兴趣可以参考博客:**CART-分类和回归树**
- https://blog.csdn.net/guoziqing506/article/details/81675022

# 2 CART算法

## 2.1 基尼不纯度gini impurity
- **或者称为基尼指数gini index**
- **区别于基尼系数gini coefficient, 两者概念不同**

假设有K个类，样本点属于第k类的概率为$p_{k}$，则概率分布的基尼指数定义为：
$$
G(p)=\sum_{k=1}^{K}p_k(1-p_k)=1-\sum_{k=1}^Kp_k^2
$$
满足的条件:
$$
\sum_{k=1}^{K}p_k=1
$$

### 2.1.1 基尼指数公式的推导

$-logp(x)$进行泰勒展开,$p(x)$的高阶趋于0,忽略高阶项.就得到基尼指数(不纯度)的公式
- 基尼不纯度的计算可以看出，它的计算更加方便，
- 基尼不纯度是熵的一个近似值

### 2.1.2 二分类的基尼指数

对于二分类问题，如果样本点属于第一类的概率为p,则概率分布的基尼系数为

$$
Gini(p)=2p(1-p)
$$

设$C_k$为D中属于第k类的样本子集，则基尼指数为
$$
Gini(D)=1-\sum_{k=1}^K(\frac{|C_k|}{|D|})^2
$$

设条件A将样本D切分为D1和D2两个数据子集，则在条件A下的样本D的基尼指数为：
$$
Gini(D,A)=\frac{|D_1|}{D}Gini(D_1)+\frac{|D_2|}{D}Gini(D_2)
$$

## 2.2 CART分类树

条件A, 将样本D, 切分为D1和D2两个数据子集的gini增益为
$$
\Delta Gini(A)=Gini(D)-Gini(D,A)=(1-\sum_{k=1}^K(\frac{|C_k|}{|D|})^2)-(\frac{|D_1|}{D}Gini(D_1)+\frac{|D_2|}{D}Gini(D_2))
$$

### 2.2.1 算法实现步骤

- 1）计算现有样本D的基尼指数，之后利用样本中每一个特征A，及A的每一个可能取值a，根据A>=a与A<a将样本分为两部分，并计算Gini(D,A)值 
- 2）找出对应基尼指数最小Gini(D,A)的最优切分特征及取值，并判断是否切分停止条件，否，则输出最优切分点 
- 3）递归调用1）2） 
- 4）生成CART决策树

## 2.3 CART回归树

### 2.3.1 CART回归树的概念和公式

- (1)训练集: $D = \{(X_1, y_1), (X_2, y_2), \dots, (X_n, y_n)\}$, $Y$是连续变量

- (2)输入数据空间$X$划分为m个区域: $\{R_1, R_2, \dots, R_m\}$

- (3)然后赋给每个输入空间的区域$R_i$有一个固定的代表输出值$C_i$

- (4)回归树的模型公式:
$$
f(X) = \sum_{i = 1}^m C_i I(X \in R_i)
$$
     * 如果$X \in R_i$,则$I=1$,否则 $I=0$
     * **含义:**先判断X属于哪个区域，然后返回这个区域的代表值。

- (5)计算损失函数:
    * $R_i$这个区域中的元组的y值的均值
$$
g_i = \frac{1}{N_i} \sum_{X_j \in R_i} y_j
$$
    * 某个区域$R_i$回归模型的损失函数
$$
J(C) = \sum_{X_j \in R_i} (f(X_j) - g_i)^2
$$

### 2.3.2 最小二乘回归树生成算法

**注**: **参考李航的<机器学习>编写, 更详细内容,请自行搜索资料查看**

- (1)选择最优切分变量 j 与切分点 s，求解
$$
\min_{j,s}\left [\min_{c_1}\sum_{x_i\in R_1(j,s)}(y_i-c_1)^2+\min_{c_2}\sum_{x_i \in R_2(j,s)} (y_i-c_2)^2 \right]
$$

- (2)用选定的对 (j,s) 划分区域并决定相应的输出值
$$
R_1(j,s)=\{x|x^{(j)} \le s\},\quad R_2(j,s)=\{x|x^{(j)}\gt s\}
$$

- (3)继续对两个子区域调用步骤(1),(2)，直至满足停止条件

- (4)将输入空间分为 M 个区域$R_1,R_2,\cdots,R_M$,生成决策树
$$
f(x)=\sum_{m=1}^M \hat c_m I(x\in R_m)
$$