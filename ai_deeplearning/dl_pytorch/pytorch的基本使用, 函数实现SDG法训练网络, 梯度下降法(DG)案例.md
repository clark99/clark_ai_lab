# 1 pytorchçš„æ¦‚å¿µ

## 1.1 tensorå¼ é‡

### 1.1.1 PyTorchçš„tensorä¸NumPy arrayç›¸äº’è½¬æ¢
PyTorchçš„å¾ˆå¤šæ“ä½œå’Œnumpyéƒ½æ˜¯ç±»ä¼¼çš„ï¼Œä½†æ˜¯å› ä¸ºå…¶èƒ½å¤Ÿåœ¨ GPU ä¸Šè¿è¡Œï¼Œæ‰€ä»¥æ¯” NumPy å¿«å¾ˆå¤šã€‚


```python
import torch
import numpy as np
# åˆ›å»ºä¸€ä¸ª numpy ndarray
numpy_tensor = np.random.randn(10, 20)
x = torch.randn(10, 20)
```

#### 1.1.1.1 ndarray==>tensor
- torch.Tensor å¼ºåˆ¶ç±»å‹è½¬æ¢
- torch.from_numpyåˆ›å»ºå‡½æ•°


```python
pytorch_tensor1 = torch.Tensor(numpy_tensor)
pytorch_tensor2 = torch.from_numpy(numpy_tensor)
print(type(pytorch_tensor2))
print(type(pytorch_tensor1))
```

    <class 'torch.Tensor'>
    <class 'torch.Tensor'>
    

#### 1.1.1.2 tensor==>ndarray


```python
# å¦‚æœ pytorch tensor åœ¨ cpu ä¸Š
numpy_array = pytorch_tensor1.numpy()

# å¦‚æœ pytorch tensor åœ¨ gpu ä¸Š
numpy_array = pytorch_tensor1.cpu().numpy()
```

### 1.1.2 PyTorch Tensor ä½¿ç”¨ GPU åŠ é€Ÿ


```python
# ç¬¬ä¸€ç§æ–¹å¼æ˜¯å®šä¹‰ cuda æ•°æ®ç±»å‹
dtype = torch.cuda.FloatTensor # å®šä¹‰é»˜è®¤ GPU çš„ æ•°æ®ç±»å‹
gpu_tensor = torch.randn(10, 20).type(dtype)

# ç¬¬äºŒç§æ–¹å¼æ›´ç®€å•ï¼Œæ¨èä½¿ç”¨
gpu_tensor = torch.randn(10, 20).cuda() # å°† tensor æ”¾åœ¨GPU ä¸Š
```

### 1.1.3 tensorçš„å±æ€§


```python
# å¯ä»¥é€šè¿‡ä¸‹é¢ä¸¤ç§æ–¹å¼å¾—åˆ° tensor çš„å¤§å°
print(pytorch_tensor1.shape)
print(pytorch_tensor1.size())
```


```python
# å¾—åˆ° tensor çš„æ•°æ®ç±»å‹
print(pytorch_tensor1.type())
```


```python
## ç»´åº¦
print(pytorch_tensor1.dim())
```


```python
# å¾—åˆ° tensor çš„æ‰€æœ‰å…ƒç´ ä¸ªæ•°
print(pytorch_tensor1.numel())
```

### 1.1.4 tensorçš„æ•°æ®ç±»å‹å˜æ¢


```python
x = torch.randn(3, 2)
print(x)
x = x.type(torch.DoubleTensor)
print(x)
x_array = x.numpy()
print(x_array.dtype)
```

## 1.2 tensorçš„æ“ä½œ

### 1.2.1 squeezeå’Œunsqueezeæ“ä½œ: é™ç»´å‡ç»´


```python
print(torch.ones(2, 2))
#torch.Size([2, 2])
print(torch.ones(2, 2).size())
x = torch.ones(2, 2).unsqueeze(0)
torch.Size([1, 2, 2])
print(x) 
print(x.size())

# å°† tensor ä¸­æ‰€æœ‰çš„ä¸€ç»´å…¨éƒ¨éƒ½å»æ‰
x = x.squeeze() 
print(x)
print(x.shape)
```

### 1.2.2 æ•°å€¼ç±»å‹è½¬æ¢


```python
# å°†å…¶è½¬åŒ–ä¸ºæ•´å½¢
x = x.long()
# x = x.type(torch.LongTensor)
print(x.type())
```

### 1.2.3 ä½¿ç”¨permuteå’Œtransposeè¿›è¡Œç»´åº¦äº¤æ¢


```python
x = torch.randn(3, 4, 5)
print(x.shape)

# ä½¿ç”¨permuteå’Œtransposeè¿›è¡Œç»´åº¦äº¤æ¢
x = x.permute(1, 0, 2) # permute å¯ä»¥é‡æ–°æ’åˆ— tensor çš„ç»´åº¦
print(x.shape)

x = x.transpose(0, 2)  # transpose äº¤æ¢ tensor ä¸­çš„ä¸¤ä¸ªç»´åº¦
print(x.shape)
```

### 1.2.4 ä½¿ç”¨ view å¯¹ tensor è¿›è¡Œ reshape


```python
x = torch.randn(3, 4, 5)
print(x.shape)
## æ‹‰ä¼¸
x = x.view(-1, 5) # -1 è¡¨ç¤ºä»»æ„çš„å¤§å°ï¼Œ5 è¡¨ç¤ºç¬¬äºŒç»´å˜æˆ 5
print(x.shape)
x = x.view(3, 20) # é‡æ–° reshape æˆ (3, 20) çš„å¤§å°
print(x.shape)
```

### 1.2.5 tensorçš„è¿ç®—:ç›¸åŠ 


```python
x = torch.zeros(3, 4)
y = torch.ones(3, 4)
# ä¸¤ä¸ª tensor æ±‚å’Œ
z = x + y
print(z)
z = torch.add(x, y)
print(z)
```

### 1.2.6 tensorçš„inplaceæ“ä½œ
pytorchä¸­å¤§å¤šæ•°çš„æ“ä½œéƒ½æ”¯æŒ inplace æ“ä½œï¼Œä¹Ÿå°±æ˜¯å¯ä»¥ç›´æ¥å¯¹ tensor è¿›è¡Œæ“ä½œè€Œä¸éœ€è¦å¦å¤–å¼€è¾Ÿå†…å­˜ç©ºé—´ã€‚æ–¹å¼éå¸¸ç®€å•ï¼Œä¸€èˆ¬éƒ½æ˜¯åœ¨æ“ä½œçš„ç¬¦å·åé¢åŠ _

## 1.3 å˜é‡

- from torch.autograd import Variable
- Variable æ˜¯å¯¹ tensor çš„å°è£…ï¼Œæ“ä½œå’Œ tensor æ˜¯ä¸€æ ·çš„ï¼Œ
- ä½†æ˜¯æ¯ä¸ª Variabeléƒ½æœ‰ä¸‰ä¸ªå±æ€§ï¼ŒVariable ä¸­çš„`.data`ï¼Œæ¢¯åº¦`.grad`ä»¥åŠè¿™ä¸ª Variable æ˜¯é€šè¿‡ä»€ä¹ˆæ–¹å¼å¾—åˆ°çš„`.grad_fn`ã€‚

### 1.3.1 å˜é‡çš„æ¢¯åº¦


```python
# é€šè¿‡ä¸‹é¢è¿™ç§æ–¹å¼å¯¼å…¥ Variable
from torch.autograd import Variable
x_tensor = torch.randn(10, 5)
y_tensor = torch.randn(10, 5)

# å°† tensor å˜æˆ Variable
# é»˜è®¤ Variable æ˜¯ä¸éœ€è¦æ±‚æ¢¯åº¦çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬ç”¨è¿™ä¸ªæ–¹å¼ç”³æ˜éœ€è¦å¯¹å…¶è¿›è¡Œæ±‚æ¢¯åº¦
x = Variable(x_tensor, requires_grad=True) 
```

# 2 æ¢¯åº¦ä¸‹é™æ³•Gradient Descent (GD)ä»‹ç»

## 2.1 æ¢¯åº¦ä¸‹é™æ³•ç®€ä»‹

### 2.1.1 æ¢¯åº¦

æ¯”å¦‚ä¸€ä¸ªä¸€ä¸ªå‡½æ•°$f(x, y)$ï¼Œé‚£ä¹ˆ $f$ çš„æ¢¯åº¦å°±æ˜¯ 

$$
(\frac{\partial f}{\partial x},\ \frac{\partial f}{\partial y})
$$

å¯ä»¥ç§°ä¸º $grad f(x, y)$ æˆ–è€… $\nabla f(x, y)$ã€‚å…·ä½“æŸä¸€ç‚¹ $(x_0,\ y_0)$ çš„æ¢¯åº¦å°±æ˜¯ $\nabla f(x_0,\ y_0)$ã€‚

### 2.1.2 å­¦ä¹ ç‡

- æ¥æ²¿ç€æ¢¯åº¦çš„åæ–¹å‘ï¼Œæˆ‘ä»¬èƒ½å¤Ÿæ›´å¿«åœ°æ‰¾åˆ°å‡½æ•°çš„æœ€å°å€¼ç‚¹
- æˆ‘ä»¬éœ€è¦æ¯æ¬¡å¾€ä¸‹èµ°çš„é‚£ä¸€æ­¥çš„é•¿åº¦ï¼Œè¿™ä¸ªé•¿åº¦ç§°ä¸ºå­¦ä¹ ç‡ï¼Œç”¨  ğœ‚  è¡¨ç¤º
- å­¦ä¹ ç‡å¤ªå°ä¼šå¯¼è‡´ä¸‹é™éå¸¸ç¼“æ…¢
- å­¦ä¹ ç‡å¤ªå¤§åˆä¼šå¯¼è‡´è·³åŠ¨éå¸¸æ˜æ˜¾

### 2.1.3 æ›´æ–°æ¢¯åº¦å…¬å¼

$$ 
w_{i+1}= w_{i} - \eta \frac{\partial f(w)}{\partial w}
$$
- 1 $ w_{i+1}$æ˜¯ç¬¬t+1æ­¥çš„ä½ç½®,<br>
- 2 $\frac{\partial f(w)}{\partial w} $ æ˜¯å¾®åˆ†,<br>
- 3 $\eta$æ˜¯æ­¥é•¿

## 2.2 æ¢¯é˜Ÿä¸‹é™çš„ç¼ºç‚¹

- éç‚¹
    - ä¸€ä¸ªä¸æ˜¯å±€éƒ¨æœ€å°å€¼çš„é©»ç‚¹ï¼ˆä¸€é˜¶å¯¼æ•°ä¸º0çš„ç‚¹ï¼‰ç§°ä¸ºéç‚¹ã€‚
    - æ•°å­¦å«ä¹‰æ˜¯ï¼š ç›®æ ‡å‡½æ•°åœ¨æ­¤ç‚¹ä¸Šçš„æ¢¯åº¦ï¼ˆä¸€é˜¶å¯¼æ•°ï¼‰å€¼ä¸º 0ï¼Œ ä½†ä»æ”¹ç‚¹å‡ºå‘çš„ä¸€ä¸ªæ–¹å‘æ˜¯å‡½æ•°çš„æå¤§å€¼ç‚¹ï¼Œè€Œåœ¨å¦ä¸€ä¸ªæ–¹å‘æ˜¯å‡½æ•°çš„æå°å€¼ç‚¹

éç‚¹çš„ä¾‹å­ $z = x^{2} - y^{2}$, å‡½æ•°å›¾å½¢ä¸º

![image.png](attachment:image.png)

$z = x^{4} - y^{3} $ çš„(0,0)ä¹Ÿæ˜¯éç‚¹

## 2.3 Stochastic Gradient Descent (SGD) ç®—æ³•

### 2.3.1 SGDçš„ç†è§£
- ç”¨ä¸€å¼ å›¾æ¥è¡¨ç¤ºï¼Œå…¶å®SGDå°±åƒæ˜¯å–é†‰äº†é…’çš„GDï¼Œå®ƒä¾ç¨€è®¤å¾—è·¯ï¼Œæœ€åä¹Ÿèƒ½è‡ªå·±èµ°å›å®¶ï¼Œä½†æ˜¯èµ°å¾—æ­ªæ­ªæ‰­æ‰­
- è™½ç„¶åŒ…å«ä¸€å®šçš„éšæœºæ€§ï¼Œä½†æ˜¯ä»æœŸæœ›ä¸Šæ¥çœ‹ï¼Œå®ƒæ˜¯ç­‰äºæ­£ç¡®çš„å¯¼æ•°çš„ï¼

### 2.3.2 SGDçš„å…¬å¼
$$
    w_{i+1}= w_{i} - \eta J
$$
- $ w_{i+1}$æ˜¯ç¬¬t+1æ­¥çš„ä½ç½®,$J$ æ˜¯éšæœºå¾®åˆ†, $\eta$æ˜¯æ­¥é•¿
- $J$çš„æœŸæœ›æ»¡è¶³æ¢¯é˜Ÿå‡½æ•°
$$
E[J]= \frac{\partial f(w)}{\partial w}
$$

# 3 pytorchå‡½æ•°å®ç°æ¢¯åº¦ä¸‹é™æ³•: çº¿æ€§å›å½’

## 3.1 çº¿æ€§å›å½’çš„å‚æ•°çš„è¯¯å·®æ›´æ–°å…¬å¼

### 3.1.1 çº¿æ€§å›å½’çš„å…¬å¼:
$$
\hat{y}_i = w x_i + b
$$
$\hat{y}_i$ æ˜¯æˆ‘ä»¬é¢„æµ‹çš„ç»“æœï¼Œå¸Œæœ›é€šè¿‡ $\hat{y}_i$ æ¥æ‹Ÿåˆç›®æ ‡ $y_i$ï¼Œé€šä¿—æ¥è®²å°±æ˜¯æ‰¾åˆ°è¿™ä¸ªå‡½æ•°æ‹Ÿåˆ $y_i$ ä½¿å¾—è¯¯å·®æœ€å°ï¼Œå³æœ€å°åŒ–æŸå¤±å‡½æ•°å®šä¹‰ä¸º

$$
J=\frac{1}{n} \sum_{i=1}^n(\hat{y}_i - y_i)^2
$$
### 3.1.2 æ¢¯é˜Ÿä¸‹é™
   $J$å¯¹$w,b$æ±‚åå¯¼, å¾®åˆ†å¾—åˆ° ${w}_{i+1}$ å’Œ ${w}_i$çš„å…³ç³»,${b}_{i+1}$ å’Œ ${b}_i$çš„å…³ç³»å¦‚ä¸‹

$$
w := w - \eta \frac{\partial f(w,\ b)}{\partial w} \\
b := b - \eta \frac{\partial f(w,\ b)}{\partial b}
$$
é€šè¿‡ä¸æ–­åœ°è¿­ä»£æ›´æ–°ï¼Œæœ€ç»ˆæˆ‘ä»¬èƒ½å¤Ÿæ‰¾åˆ°ä¸€ç»„æœ€ä¼˜çš„ w å’Œ bï¼Œè¿™å°±æ˜¯æ¢¯åº¦ä¸‹é™æ³•çš„åŸç†ã€‚
### 3.1.3 çº¿æ€§å›å½’çš„æ¢¯åº¦ä¸‹é™
w å’Œ b çš„æ¢¯åº¦åˆ†åˆ«æ˜¯

$$
\frac{\partial J}{\partial w} = \frac{2}{n} \sum_{i=1}^n x_i(w x_i + b - y_i) \\
\frac{\partial J}{\partial b} = \frac{2}{n} \sum_{i=1}^n (w x_i + b - y_i)
$$


## 3.2 ä»£ç å®ç°


```python
import torch
import numpy as np
from torch.autograd import Variable
import matplotlib.pyplot as plt
%matplotlib inline
# å®šä¹‰éšæœºå› å­
torch.manual_seed(2019)
```




    <torch._C.Generator at 0x2333a2f2c10>



### 3.2.1 åˆ›å»ºå¼ é‡tensor(æµ‹è¯•æ ·æœ¬æ•°æ®)
tensorçš„ä½¿ç”¨æ¥å£å’Œ numpy éå¸¸ç›¸ä¼¼


```python
x_train = np.array([[3.3], [4.4], [5.5], [6.71], [6.93], [4.168],
                    [9.779], [6.182], [7.59], [2.167], [7.042],
                    [10.791], [5.313], [7.997], [3.1]], dtype=np.float32)

y_train = np.array([[1.7], [2.76], [2.09], [3.19], [1.694], [1.573],
                    [3.366], [2.596], [2.53], [1.221], [2.827],
                    [3.465], [1.65], [2.904], [1.3]], dtype=np.float32)
# è½¬æ¢æˆ Tensor
x_train = torch.from_numpy(x_train)
y_train = torch.from_numpy(y_train)
```

### 3.2.2  åˆ›å»ºå˜é‡Variable
- from torch.autograd import Variable
- Variable æ˜¯å¯¹ tensor çš„å°è£…ï¼Œæ“ä½œå’Œ tensor æ˜¯ä¸€æ ·çš„ï¼Œ
- ä½†æ˜¯æ¯ä¸ª Variabeléƒ½æœ‰ä¸‰ä¸ªå±æ€§ï¼ŒVariable ä¸­çš„.dataï¼Œæ¢¯åº¦.gradä»¥åŠè¿™ä¸ª Variable æ˜¯é€šè¿‡ä»€ä¹ˆæ–¹å¼å¾—åˆ°çš„.grad_fn


```python
# å®šä¹‰å‚æ•° w å’Œ b
w = Variable(torch.randn(1), requires_grad=True) # éšæœºåˆå§‹åŒ–
b = Variable(torch.zeros(1), requires_grad=True) # ä½¿ç”¨ 0 è¿›è¡Œåˆå§‹åŒ–
x_train = Variable(x_train)
y_train = Variable(y_train)
print(w)
print(b)
```

    tensor([-0.1187], requires_grad=True)
    tensor([0.], requires_grad=True)
    

### 3.2.3 æ„å»ºè®¡ç®—å›¾: æ‹Ÿåˆæ¨¡å‹,æŸå¤±å‡½æ•°


```python
def linear_model(x):
    return x * w + b
```


```python
# è®¡ç®—è¯¯å·®
def get_loss(y_, y):
    return torch.mean((y_ - y_train) ** 2)
```

### 3.2.4 è®­ç»ƒæ¨¡å‹

#### 3.2.4.1 è®­ç»ƒæ¨¡å‹-æŸ¥çœ‹åˆå§‹åŒ–å‚æ•°çš„æ¨¡å‹

- ç›®çš„:
    * æ ¹æ®åˆå§‹è¶…å‚æ•°, è®¡ç®—ç¬¬ä¸€æ¬¡ä¼°è®¡çš„yå€¼
    * æ ¹æ®ç¬¬ä¸€æ¬¡ä¼°ç®—çš„å€¼, è®¡ç®—è¯¯å·®
- ç”»å›¾
    * tensorçš„dataå±æ€§
    * pltçš„è¾“å…¥ä¸ºnumpyç±»å‹


```python
y_ = linear_model(x_train)
```


```python
loss = get_loss(y_, y_train)
# æ‰“å°ä¸€ä¸‹çœ‹çœ‹ loss çš„å¤§å°
print(loss)
```

    tensor(10.2335, grad_fn=<MeanBackward1>)
    


```python
plt.plot(x_train.data.numpy(), y_train.data.numpy(), 'bo', label='real')
plt.plot(x_train.data.numpy(), y_.data.numpy(), 'ro', label='estimated')
plt.legend()
```




    <matplotlib.legend.Legend at 0x2333d3d9358>




![png](output_65_1.png)


#### 3.2.4.2 è®­ç»ƒæ¨¡å‹-ç¬¬ä¸€æ¬¡é€šè¿‡æŸå¤±å‡½æ•°çš„æ¢¯é˜Ÿæ¥ä¼˜åŒ–å‚æ•°,ä¼˜åŒ–æ¨¡å‹

**æ³¨:** ç¬¬ä¸€æ¬¡ä¸éœ€è¦æ¢¯åº¦å½’é›¶ grad.zero_()
å¦åˆ™None,'NoneType' object has no attribute 'zero_'



```python
## è®¡ç®—w,bæ¢¯é˜Ÿå¾®åˆ†
# å°† tensor å˜æˆ Variable 
# é»˜è®¤ Variable æ˜¯ä¸éœ€è¦æ±‚æ¢¯åº¦çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬ç”¨è¿™ä¸ªæ–¹å¼ç”³æ˜éœ€è¦å¯¹å…¶è¿›è¡Œæ±‚æ¢¯åº¦
# æ ¹æ®tensorçš„gent_lossæŸå¤±å‡½æ•°è®¡ç®—å›¾ä¸­, x,yå·²çŸ¥çš„æŸå¤±å€¼tensorå¯¹è±¡Loss
## è‡ªåŠ¨å¯¹tensorå¯¹è±¡æ±‚æ±‚å‚æ•°çš„åå¯¼å‡½æ•°,å¹¶ä¸”è®¡ç®—æ¢¯é˜Ÿå¾®åˆ†
#x = Variable(x_tensor, requires_grad=True) 
#y = Variable(y_tensor, requires_grad=True)
print(loss)
loss.backward()
print(w.grad)
print(b.grad)
```

    tensor(10.2335, grad_fn=<MeanBackward1>)
    None
    tensor([-41.1289])
    tensor([-6.0890])
    

#### 3.2.4.3 æ ¹æ®å­¦ä¹ ç‡å’Œè®¡ç®—çš„w,bçš„æ¢¯é˜Ÿæ¥æ›´æ–°w,bçš„å‚æ•°


```python
# æ¢¯é˜Ÿä¸‹é™,å‡å»å­¦ä¹ ç‡*æ¢¯é˜Ÿå¾®åˆ†, æ›´æ–°ä¸€æ¬¡å‚æ•°
w.data = w.data - 1e-2 * w.grad.data
b.data = b.data - 1e-2 * b.grad.data
y_ = linear_model(x_train)
plt.plot(x_train.data.numpy(), y_train.data.numpy(), 'bo', label='real')
plt.plot(x_train.data.numpy(), y_.data.numpy(), 'ro', label='estimated')
plt.legend()
```




    <matplotlib.legend.Legend at 0x2333d45ef28>




![png](output_70_1.png)


#### 3.2.4.4 è¿­ä»£30æ¬¡,è®­ç»ƒæ¨¡å‹çš„å‚æ•°


```python
for e in range(31): # è¿›è¡Œ 30 æ¬¡æ›´æ–°
    y_ = linear_model(x_train) ## è®¡ç®—æ‹Ÿåˆçš„Yå€¼: tensor
    loss = get_loss(y_, y_train) ## è®¡ç®—æŸå¤±å€¼: tensor
    w.grad.zero_() # è®°å¾—å½’é›¶æ¢¯åº¦
    b.grad.zero_() # è®°å¾—å½’é›¶æ¢¯åº¦
    # è‡ªåŠ¨æ±‚å¯¼,è®¡ç®—æ¢¯é˜Ÿw.grad.data,b.grad.data
    loss.backward()
    # ä½¿ç”¨æ¢¯é˜Ÿæ›´æ–°å‚æ•°
    w.data = w.data - 1e-2 * w.grad.data # æ›´æ–° w
    b.data = b.data - 1e-2 * b.grad.data # æ›´æ–° b 
    if e%10==0:
        print('epoch: {}, loss: {}'.format(e, loss.item()))
```

    epoch: 0, loss: 0.4142104387283325
    epoch: 10, loss: 0.2260516732931137
    epoch: 20, loss: 0.2231873869895935
    epoch: 30, loss: 0.2204667031764984
    


```python
y_ = linear_model(x_train)
plt.plot(x_train.data.numpy(), y_train.data.numpy(), 'bo', label='real')
plt.plot(x_train.data.numpy(), y_.data.numpy(), 'ro', label='estimated')
plt.legend()
```




    <matplotlib.legend.Legend at 0x2333d4ef550>




![png](output_73_1.png)


# 4 pytorchå‡½æ•°å®ç°å®ç°SGD: mnistæ‰‹å†™ä½“è¯†åˆ«

## 4.1 åŠ è½½æ•°æ®


```python
import numpy as np
import torch
from torchvision.datasets import MNIST # å¯¼å…¥ pytorch å†…ç½®çš„ mnist æ•°æ®
from torch.utils.data import DataLoader
from torch import nn
from torch.autograd import Variable
import time
import matplotlib.pyplot as plt
%matplotlib inline
```

### 4.1.1 æ•°æ®è½¬æ¢è§„èŒƒ


```python
def transform_mnist(x):
    x = np.array(x, dtype='float32') / 255 # å°†æ•°æ®å˜åˆ° 0 ~ 1 ä¹‹é—´
    x = (x - 0.5) / 0.5 # æ ‡å‡†åŒ–ï¼Œè¿™ä¸ªæŠ€å·§ä¹‹åä¼šè®²åˆ°
    x = x.reshape((-1,)) # æ‹‰å¹³
    x = torch.from_numpy(x)
    return x
```

### 4.1.2 ä»ä¸‹è½½çš„æ–‡ä»¶ä¸­åŠ è½½æ•°æ®


```python
def download_mnist_data(batch_size):
    # transform å‡½æ•°å¼ç¼–ç¨‹, å¯¹æ•°æ®çš„å˜æ¢ trainæŒ‡å®šæµ‹è¯•è®­ç»ƒ
    # è½½å…¥æ•°æ®é›†ï¼Œç”³æ˜å®šä¹‰çš„æ•°æ®å˜æ¢
    # å®šä¹‰train=True or false
    train_set = MNIST(r'E:\ai\ai_lab\ai_case\ai_data\pytorch\MNIST\data', train=True, transform=transform_mnist, download=False) 
    test_set = MNIST(r'E:\ai\ai_lab\ai_case\ai_data\pytorch\MNIST\data', train=False, transform=transform_mnist, download=False)
    # åŠ è½½æ•°æ®,æ‰“ä¹±æ•°æ®
    ## DataLoader: æ‰¹é‡å–å¤šå°‘å›¾ç‰‡
    train_data = DataLoader(train_set, batch_size=batch_size, shuffle=True)
    return train_data
```

## 4.2 å®šä¹‰ç½‘ç»œç»“æ„,å®šä¹‰æŸå¤±å‡½æ•°,å®šä¹‰SDGå‡½æ•°

### 4.2.1 ä½¿ç”¨ Sequential å®šä¹‰ 3 å±‚ç¥ç»ç½‘ç»œ


```python
# æœ€åä¸€å±‚10ä¸ªè¾“å‡º
net = nn.Sequential(
        nn.Linear(784, 200),
        nn.ReLU(),
        nn.Linear(200, 10),
)
```

### 4.2.2 å®šä¹‰æŸå¤±å‡½æ•°


```python
# äº¤å‰ç†µå‡½æ•°
criterion = nn.CrossEntropyLoss()
```

### 4.2.3 å®šä¹‰SDGå‡½æ•°


```python
def sgd_update(parameters, lr):
    for param in parameters:
        param.data = param.data - lr * param.grad.data
```

## 4.3 è®­ç»ƒç½‘ç»œ

### 4.3.1 å®šä¹‰ç½‘ç»œè®­ç»ƒçš„å‡½æ•°è¿‡ç¨‹

**æ³¨1: æ¢¯é˜Ÿå½’é›¶çš„æ–¹æ³•**
- ç›´æ¥æŠŠæ¨¡å‹çš„å‚æ•°æ¢¯åº¦è®¾æˆ0:
    * model.zero_grad()
    * optimizer.zero_grad() 
    * å½“optimizer=optim.Optimizer(model.parameters())æ—¶ï¼Œä¸¤è€…ç­‰æ•ˆ
- å¦‚æœæƒ³è¦æŠŠæŸä¸€Variableçš„æ¢¯åº¦ç½®ä¸º0
    * Variable.grad.data.zero_()


```python
def train_net(net,train_data,lr):
    train_losses=[]
    train_loss = 0
    losses_idx = 0
    ### batch_sizeå½±å“im,labelçš„å¤§å°
    ### batch_sizeå½±å“è¿­ä»£æ¬¡æ•°.
    for im, label in train_data:
        # å®šä¹‰å˜é‡:
        im = Variable(im)
        label = Variable(label)
        # å‰å‘ä¼ æ’­,ä¼ å…¥è¾“å…¥tensor,è·å¾—è¾“å‡ºtensor
        out = net(im)
        # è®¡ç®—æŸå¤±tensor: æ ¹æ®å‰å‘ä¼ æ’­çš„é¢„æµ‹å€¼å’ŒçœŸæ˜¯æ ‡ç­¾
        loss = criterion(out, label)
        # å°†ç½‘ç»œçš„æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦å±æ€§ç½®ä¸ºé›¶
        net.zero_grad()
        # é»˜è®¤ Variable æ˜¯ä¸éœ€è¦æ±‚æ¢¯åº¦çš„ï¼Œ requires_grad=True è¿™ä¸ªæ–¹å¼ç”³æ˜éœ€è¦å¯¹å…¶è¿›è¡Œæ±‚æ¢¯åº¦
        # æŸå¤±å€¼tensorçš„å¯¹ç½‘ç»œå‚æ•°çš„åå¯¼
        loss.backward()
        ## å¯¹æ¨¡å‹å‚æ•°æ›´æ–°,
        ## å±æ€§ net.parameterså¾—åˆ°æ‰€æœ‰å‚æ•°
        sgd_update(net.parameters(),lr) # ä½¿ç”¨lrçš„å­¦ä¹ ç‡
        # è®°å½•è¯¯å·®
        train_loss += loss.item()
        if losses_idx % 10 == 0:
            train_losses.append(loss.item())
        losses_idx += 1
    return (train_losses,train_loss,losses_idx)
```

### 4.3.2 å®šä¹‰ä¸»å‡½æ•°


```python
def train(net,train_data,lr,batch_size):
    start = time.time() # è®°æ—¶å¼€å§‹
    # è®­ç»ƒç½‘ç»œ,è®°å½•æŸå¤±å€¼
    (train_losses,train_loss,losses_idx)=train_net(net,train_data,lr)
    end = time.time() # è®¡æ—¶ç»“æŸ
    print('epoch: 1, Train Loss: {:.6f}'.format(train_loss / len(train_data)))
    print('ä½¿ç”¨æ—¶é—´: {:.5f} s'.format(end - start))
    x_axis = np.linspace(0, 5, len(train_losses), endpoint=True)
    plt.semilogy(x_axis, train_losses, label='batch_size=%d'%batch_size)
    plt.legend(loc='best')
```

### 4.3.3 å¼€å§‹è®­ç»ƒç½‘ç»œ

å®šä¹‰å­¦ä¹ ç‡å’Œæ‰¹é‡è§„æ¨¡


```python
batch_size=100
lr=1e-2
```

ä¸‹è½½æ•°æ®, ä½¿ç”¨torch.utils.data.DataLoaderæ–¹æ³•æ‰¹é‡åŠ è½½æ•°æ®


```python
train_data=download_mnist_data(batch_size=batch_size)
```

è®­ç»ƒæ•°æ®, æ ¹æ®æ‰¹é‡è§„æ¨¡, è®­ç»ƒæ‰€æœ‰çš„æ•°æ®


```python
train(net,train_data,lr=1e-2,batch_size=100)
```

    epoch: 1, Train Loss: 0.905261
    ä½¿ç”¨æ—¶é—´: 7.80006 s
    


![png](output_100_1.png)



```python

```
